{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1899f903",
   "metadata": {},
   "source": [
    "# Poultry Disease Classification from Fecal Images\n",
    "\n",
    "This notebook classifies poultry diseases based on fecal images into four classes:\n",
    "\n",
    "- **Healthy**\n",
    "- **Coccidiosis (cocci)**\n",
    "- **Salmonella (salmo)**\n",
    "- **Newcastle Disease (ncd)**\n",
    "\n",
    "> âš™ï¸ *The model is optimized for Mac M1 with 8GB RAM.* (Farhan Mashrur)\n",
    "- Model initially developed with Ahmed Abdulla (Teammate) for Mac M2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fc4e4",
   "metadata": {},
   "source": [
    "### GPU availability Testing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f19079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farhanmashrur/Desktop/cds/avian_alert/tf-metal-env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6b22a",
   "metadata": {},
   "source": [
    " ## 1. Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3bde5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Enable mixed precision for Apple Silicon (M1/M2)\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Modules loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811654e8",
   "metadata": {},
   "source": [
    "## 2. Enable GPU Acceleration (for Mac M1)\n",
    "#### For Mac M1, we are using TensorFlow-MacOS and Metal plugin are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a490c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU acceleration enabled on M1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU acceleration enabled on M1\")\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "except Exception as e:\n",
    "    print(\"GPU acceleration not available:\", e)\n",
    "\n",
    "\n",
    "# 3. Image Settings for Mac M1 (8GB RAM)\n",
    "IMG_SIZE = (160, 160)  # Reduced for efficiency\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32        # Lower to 16 if memory issues occur\n",
    "IMG_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b559392",
   "metadata": {},
   "source": [
    "## 3) Custom Callback for training and Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d2d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience=1, stop_patience=3, threshold=0.9, factor=0.5, batches=None, epochs=None):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self._model = model\n",
    "        self.patience = patience \n",
    "        self.stop_patience = stop_patience\n",
    "        self.threshold = threshold\n",
    "        self.factor = factor\n",
    "        self.batches = batches\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.count = 0\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1\n",
    "        \n",
    "        try:\n",
    "            self.current_lr = 0.001\n",
    "            if hasattr(model.optimizer, 'learning_rate'):\n",
    "                lr = model.optimizer.learning_rate\n",
    "                if hasattr(lr, 'numpy'):\n",
    "                    self.current_lr = float(lr.numpy())\n",
    "            elif hasattr(model.optimizer, 'lr'):\n",
    "                lr = model.optimizer.lr\n",
    "                if hasattr(lr, 'numpy'):\n",
    "                    self.current_lr = float(lr.numpy())\n",
    "        except:\n",
    "            self.current_lr = 0.001\n",
    "            \n",
    "        self.initial_lr = self.current_lr\n",
    "        self.highest_tracc = 0.0\n",
    "        self.lowest_vloss = np.inf\n",
    "        self.best_weights = self._model.get_weights()\n",
    "        self.initial_weights = self._model.get_weights()\n",
    "\n",
    "        print(\"âœ… Callback initialization complete.\")\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format(\n",
    "            'Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'\\nTraining time: {int(hours)}h {int(minutes)}m {seconds:.2f}s'\n",
    "        print(msg)\n",
    "        self._model.set_weights(self.best_weights)\n",
    "        print(\"âœ… Best weights restored.\")\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "        msg = f'processing batch {batch + 1} of {self.batches} - accuracy: {acc:.2f}% - loss: {loss:.5f}'\n",
    "        print(msg, '\\r', end='')\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.ep_start = time.time()\n",
    "        print(f\"\\nğŸ” Starting epoch {epoch + 1}\")\n",
    "\n",
    "    def _update_lr(self, new_lr):\n",
    "        try:\n",
    "            if hasattr(self._model.optimizer, 'learning_rate'):\n",
    "                tf.keras.backend.set_value(self._model.optimizer.learning_rate, new_lr)\n",
    "            elif hasattr(self._model.optimizer, 'lr'):\n",
    "                tf.keras.backend.set_value(self._model.optimizer.lr, new_lr)\n",
    "            self.current_lr = new_lr\n",
    "            print(f\"ğŸ“‰ Learning rate updated to {new_lr:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ Failed to update learning rate:\", e)\n",
    "        return new_lr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "        current_lr = self.current_lr\n",
    "\n",
    "        acc = logs.get('accuracy')\n",
    "        v_acc = logs.get('val_accuracy')\n",
    "        loss = logs.get('loss')\n",
    "        v_loss = logs.get('val_loss')\n",
    "        next_lr = current_lr\n",
    "\n",
    "        if acc < self.threshold:\n",
    "            monitor = 'accuracy'\n",
    "            pimprov = 0.0 if epoch == 0 else (acc - self.highest_tracc) * 100 / self.highest_tracc\n",
    "            if acc > self.highest_tracc:\n",
    "                self.highest_tracc = acc\n",
    "                self.best_weights = self._model.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    next_lr = current_lr * self.factor\n",
    "                    self._update_lr(next_lr)\n",
    "                    self.count = 0\n",
    "                    self.stop_count += 1\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count += 1\n",
    "        else:\n",
    "            monitor = 'val_loss'\n",
    "            pimprov = 0.0 if epoch == 0 else (self.lowest_vloss - v_loss) * 100 / self.lowest_vloss\n",
    "            if v_loss < self.lowest_vloss:\n",
    "                self.lowest_vloss = v_loss\n",
    "                self.best_weights = self._model.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    next_lr = current_lr * self.factor\n",
    "                    self._update_lr(next_lr)\n",
    "                    self.stop_count += 1\n",
    "                    self.count = 0\n",
    "                else:\n",
    "                    self.count += 1\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{epoch + 1:^8} {loss:^10.3f}{acc * 100:^9.2f}{v_loss:^9.5f}{v_acc * 100:^9.2f}{current_lr:^9.5f}{next_lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1:\n",
    "            print(f\"\\nğŸ›‘ Training halted at epoch {epoch + 1} â€” no improvement after {self.stop_patience} learning rate adjustments.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be9ec2",
   "metadata": {},
   "source": [
    "## Code to test callback functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b302c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:54:31.139849: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-11 00:54:31.140040: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-11 00:54:31.140051: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-11 00:54:31.140470: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-11 00:54:31.140582: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Callback initialization complete.\n",
      " Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n",
      "\n",
      "ğŸ” Starting epoch 1\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:54:31.943183: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1       2.407     22.50   2.22072   40.00   0.00100  0.00100  accuracy     0.00     2.35  ss: 1.6198processing batch 2 of 6 - accuracy: 21.88% - loss: 2.28235 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.2234 - loss: 2.2054 - val_accuracy: 0.4000 - val_loss: 2.2207\n",
      "\n",
      "ğŸ” Starting epoch 2\n",
      "Epoch 2/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2510 - loss: 2.3673processing batch 2 of 6 - accuracy: 21.88% - loss: 2.36983    2       2.163     31.25   2.45557   10.00   0.00100  0.00100  accuracy    38.89     0.18  \n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2613 - loss: 2.3332 - val_accuracy: 0.1000 - val_loss: 2.4556\n",
      "\n",
      "ğŸ” Starting epoch 3\n",
      "Epoch 3/5\n",
      "âš ï¸ Failed to update learning rate: 'str' object has no attribute 'name' - accuracy: 0.2734 - loss: 1.8801processing batch 2 of 6 - accuracy: 31.25% - loss: 1.90532 processing batch 5 of 6 - accuracy: 25.00% - loss: 1.73536 \n",
      "   3       1.735     25.00   1.81937   25.00   0.00100  0.00050  accuracy    -20.00    0.18  \n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2656 - loss: 1.8318 - val_accuracy: 0.2500 - val_loss: 1.8194\n",
      "\n",
      "ğŸ” Starting epoch 4\n",
      "Epoch 4/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1873 - loss: 1.8021processing batch 2 of 6 - accuracy: 25.00% - loss: 1.66776 âš ï¸ Failed to update learning rate: 'str' object has no attribute 'name'\n",
      "   4       1.769     21.25   1.55720   25.00   0.00100  0.00050  accuracy    -32.00    0.17  \n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1915 - loss: 1.7966 - val_accuracy: 0.2500 - val_loss: 1.5572\n",
      "\n",
      "ğŸ” Starting epoch 5\n",
      "Epoch 5/5\n",
      "âš ï¸ Failed to update learning rate: 'str' object has no attribute 'name' - accuracy: 0.2617 - loss: 1.5943processing batch 2 of 6 - accuracy: 21.88% - loss: 1.62058 processing batch 5 of 6 - accuracy: 23.75% - loss: 1.66769 \n",
      "   5       1.668     23.75   1.52533   30.00   0.00100  0.00050  accuracy    -24.00    0.18  \n",
      "\n",
      "ğŸ›‘ Training halted at epoch 5 â€” no improvement after 3 learning rate adjustments.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2536 - loss: 1.6188 - val_accuracy: 0.3000 - val_loss: 1.5253\n",
      "\n",
      "Training time: 0h 0m 3.10s\n",
      "âœ… Best weights restored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x317fb1970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Assume your callback class is already defined above as MyCallback\n",
    "\n",
    "# 1. Create dummy image data (100 samples of 32x32 RGB images)\n",
    "X_dummy = np.random.rand(100, 32, 32, 3).astype(np.float32)\n",
    "\n",
    "# 2. Create dummy labels (4 classes)\n",
    "y_dummy = np.random.randint(0, 4, 100)\n",
    "\n",
    "# 3. Build a simple model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# 4. Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Create your custom callback instance\n",
    "cb = MyCallback(model=model, epochs=5, batches=X_dummy.shape[0] // 16)\n",
    "\n",
    "# 6. Train with the dummy data\n",
    "model.fit(X_dummy, y_dummy,\n",
    "          epochs=5,\n",
    "          batch_size=16,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f5ad0",
   "metadata": {},
   "source": [
    "## Data Preparation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757661c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(zip_dir, extract_dir):\n",
    "    \"\"\"\n",
    "    Extract all zip files containing the image data\n",
    "    \"\"\"\n",
    "    # create extract directory if it doesn't exist\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    # list of expected zip files\n",
    "    expected_zips = ['healthy.zip', 'cocci.zip', 'salmo.zip', 'ncd.zip']\n",
    "    \n",
    "    for zip_file in expected_zips:\n",
    "        zip_path = os.path.join(zip_dir, zip_file)\n",
    "        if os.path.exists(zip_path):\n",
    "            print(f\"Extracting {zip_file}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # extract to a subfolder named after the class (removing .zip extension)\n",
    "                class_name = zip_file.split('.')[0]\n",
    "                class_dir = os.path.join(extract_dir, class_name)\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                zip_ref.extractall(class_dir)\n",
    "            print(f\"Extracted {zip_file} to {class_dir}\")\n",
    "        else:\n",
    "            print(f\"Warning: {zip_file} not found in {zip_dir}\")\n",
    "\n",
    "def create_csv_from_directory(data_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    Create a CSV file with image paths and labels from directory structure\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through class directories\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            # Iterate through images in the class directory\n",
    "            for image_file in os.listdir(class_dir):\n",
    "                if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    # Use relative paths so it works in different environments\n",
    "                    image_path = os.path.join(class_name, image_file)\n",
    "                    filepaths.append(image_path)\n",
    "                    labels.append(class_name)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Created CSV file with {len(df)} images\")\n",
    "    return df\n",
    "\n",
    "def split_data(data_dir, csv_path):\n",
    "    \"\"\"\n",
    "    Split the data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = ['filepaths', 'labels']\n",
    "    else:\n",
    "        df = create_csv_from_directory(data_dir, csv_path)\n",
    "    \n",
    "    # Add full paths\n",
    "    df['filepaths'] = df['filepaths'].apply(lambda x: os.path.join(data_dir, x))\n",
    "    \n",
    "    # Create train df\n",
    "    strat = df['labels']\n",
    "    train_df, dummy_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    # Valid and test dataframe\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size=0.5, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8050c",
   "metadata": {},
   "source": [
    "## Data Generation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4483ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gens(train_df, valid_df, test_df, batch_size, img_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Create image data generators for train, validation, and test sets\n",
    "    \"\"\"\n",
    "    channels = 3\n",
    "    color = 'rgb'\n",
    "    \n",
    "    # Calculate test batch size\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) \n",
    "                                  if ts_length % n == 0 and ts_length/n <= 80]))\n",
    "    \n",
    "    # Function for preprocessing\n",
    "    def scalar(img):\n",
    "        return img\n",
    "    \n",
    "    # Create generators with augmentation for training\n",
    "    tr_gen = ImageDataGenerator(\n",
    "        preprocessing_function=scalar,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        zoom_range=0.2\n",
    "    )\n",
    "    \n",
    "    ts_gen = ImageDataGenerator(preprocessing_function=scalar)\n",
    "    \n",
    "    # Flow from dataframes\n",
    "    train_gen = tr_gen.flow_from_dataframe(\n",
    "        train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    valid_gen = ts_gen.flow_from_dataframe(\n",
    "        valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    test_gen = ts_gen.flow_from_dataframe(\n",
    "        test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=False, batch_size=test_batch_size\n",
    "    )\n",
    "    \n",
    "    return train_gen, valid_gen, test_gen\n",
    "\n",
    "def show_images(gen):\n",
    "    \"\"\"\n",
    "    Show sample images from the generator\n",
    "    \"\"\"\n",
    "    g_dict = gen.class_indices\n",
    "    classes = list(g_dict.keys())\n",
    "    images, labels = next(gen)\n",
    "    \n",
    "    length = len(labels)\n",
    "    sample = min(length, 25)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])\n",
    "        class_name = classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f265e3",
   "metadata": {},
   "source": [
    "## Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9aac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    \n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    \n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label='Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label='Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized Confusion Matrix')\n",
    "    else:\n",
    "        print('Confusion Matrix, Without Normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment='center', \n",
    "                     color='white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
